import logging
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle
import re

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
MODEL = load_model('url_classifier_lstm.1h5')
with open('tokenizer.pkl1', 'rb') as f:
    TOKENIZER = pickle.load(f)
with open('label_encoder.pkl1', 'rb') as f:
    LE = pickle.load(f)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
MAX_LEN = 150
BOT_TOKEN = '–í–ê–®_TELEGRAM_BOT_TOKEN'

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)


async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
    user = update.effective_user
    await update.message.reply_text(
        f"–ü—Ä–∏–≤–µ—Ç, {user.first_name}!\n"
        "–Ø –±–æ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ URL.\n"
        "–ü—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤—å –º–Ω–µ —Å—Å—ã–ª–∫—É, –∏ —è –ø—Ä–æ–≤–µ—Ä—é –µ—ë –Ω–∞:\n"
        "‚Ä¢ –î–µ—Ñ–µ–π—Å–º–µ–Ω—Ç\n‚Ä¢ –§–∏—à–∏–Ω–≥\n‚Ä¢ –í—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–µ –ü–û\n‚Ä¢ –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ —Å–∞–π—Ç—ã"
    )


async def analyze_url(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ URL"""
    try:
        url = update.message.text

        # –í–∞–ª–∏–¥–∞—Ü–∏—è URL
        if not re.match(r'^https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+', url):
            await update.message.reply_text("‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –ø–æ–ª–Ω—É—é —Å—Å—ã–ª–∫—É —Å http/https")
            return

        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        sequence = TOKENIZER.texts_to_sequences([url])
        padded = pad_sequences(sequence, maxlen=MAX_LEN)
        proba = MODEL.predict(padded, verbose=0)[0]

        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        response = "üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:\n\n"
        for cls, prob in zip(LE.classes_, np.round(proba * 100, 1)):
            response += f"{cls.upper()}: {prob}%\n"

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–ª–∞–≤–Ω–æ–π —É–≥—Ä–æ–∑—ã
        main_threat = LE.classes_[np.argmax(proba)]
        if main_threat == 'benign':
            response += "\n‚úÖ –≠—Ç–∞ —Å—Å—ã–ª–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–∞"
        else:
            response += f"\n‚ö†Ô∏è –û—Å–Ω–æ–≤–Ω–∞—è —É–≥—Ä–æ–∑–∞: {main_threat.upper()}"

        await update.message.reply_text(response)

    except Exception as e:
        logging.error(f"Error: {e}")
        await update.message.reply_text("üòû –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥—É—é —Å—Å—ã–ª–∫—É")


def main():
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
    app = Application.builder().token(BOT_TOKEN).build()

    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤
    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, analyze_url))

    # –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
    logging.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω...")
    app.run_polling()


if __name__ == "__main__":
    main()